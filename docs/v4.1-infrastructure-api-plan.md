# Claude-Slack v4.1: Infrastructure API Implementation Plan

## Executive Summary

Claude-Slack needs to expose a programmatic Python API that allows the intelligence layer (claude-brain) to access infrastructure capabilities beyond what MCP tools provide. This document outlines the implementation required to fulfill claude-slack's role as the **unopinionated infrastructure layer** for the knowledge system.

## Design Principles

1. **Clean Public API**: Expose functionality through a well-defined Python package, not internal classes
2. **Backward Compatible**: All existing MCP tools continue to work unchanged
3. **No Intelligence**: No pattern extraction, synthesis, or curation - pure data operations
4. **Extensible**: Allow custom functions (like rankers) to be injected without modification
5. **Efficient**: Support the performance needs of the intelligence layer
6. **Schema-Agnostic**: No assumptions about metadata structure or conventions
7. **Schema-Optimized**: Apps can register schemas for performance while maintaining flexibility

## Required Implementation

### 1. Package Structure

Create a new Python package structure for the public API:

```
claude-slack/
├── claude_slack/                 # NEW: Public Python package
│   ├── __init__.py              # Package exports
│   ├── api.py                   # Main API class
│   ├── models.py                # Data models (Message, SearchResult, etc.)
│   ├── registry.py              # Schema registry and management
│   ├── transformer.py           # Nested ↔ flat transformations
│   ├── translator.py            # Query translation logic
│   ├── filters.py               # Metadata filter parsing
│   └── exceptions.py            # Custom exceptions
├── mcp/                         # Existing MCP server (unchanged)
│   ├── server.py
│   └── [existing structure]
└── setup.py                     # NEW: Package installation
```

### 2. Core API Class

```python
# claude_slack/api.py

from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass
from datetime import datetime
from .models import Message, SearchResult, AggregationResult
from .filters import MetadataFilter

class ClaudeSlackAPI:
    """
    Public API for claude-slack infrastructure.
    
    This is the primary interface for external systems (like claude-brain)
    to interact with the claude-slack knowledge store.
    
    IMPORTANT: This API is completely unopinionated about metadata structure.
    It doesn't know about "breadcrumbs", "outcomes", "patterns", or any other
    conventions. It simply stores and queries arbitrary JSON metadata.
    
    OPTIMIZATION: Apps can register their metadata schemas to enable efficient
    ChromaDB indexing while still using their natural nested structures.
    """
    
    def __init__(self, 
                 db_path: str,
                 enable_semantic: bool = True,
                 cache_embeddings: bool = True):
        """
        Initialize the API connection.
        
        Args:
            db_path: Path to SQLite database
            enable_semantic: Use ChromaDB for semantic search if available
            cache_embeddings: Cache embeddings for performance
        """
        self._db_path = db_path
        self._init_storage()
        self.registry = SchemaRegistry()
        self.transformer = SchemaTransformer(self.registry)
        self.translator = QueryTranslator(self.registry)
    
    # ========== Core Operations (Existing) ==========
    
    async def store_message(self, 
                           channel_id: str,
                           sender_id: str, 
                           content: str,
                           metadata: Optional[Dict] = None,
                           confidence: Optional[float] = None,
                           timestamp: Optional[datetime] = None) -> int:
        """Store a message with metadata."""
        pass
    
    async def search(self,
                    query: Optional[str] = None,
                    ranking_profile: str = "balanced",
                    limit: int = 20,
                    **filters) -> List[SearchResult]:
        """Basic semantic/keyword search."""
        pass
    
    async def get_message(self, message_id: int) -> Optional[Message]:
        """Retrieve a single message by ID."""
        pass
    
    # ========== NEW: Schema Registry for Performance Optimization ==========
    
    def register_schema(self,
                       schema_name: str,
                       schema_def: Dict,
                       indexed_fields: List[str],
                       options: Optional[Dict] = None) -> None:
        """
        Register a metadata schema for optimal storage and querying.
        
        This enables efficient ChromaDB indexing while apps continue using
        their natural nested JSON structures. The infrastructure handles all
        transformations transparently.
        
        Args:
            schema_name: Unique identifier for this schema (e.g., "reflection", "insight")
            schema_def: The nested structure definition (for documentation/validation)
            indexed_fields: Paths to extract for ChromaDB indexing
            options: Additional configuration (array handling, type hints, etc.)
        
        Example:
            api.register_schema(
                "reflection",
                schema_def={
                    "type": "string",
                    "outcome": "string", 
                    "confidence": "float",
                    "breadcrumbs": {
                        "task": "string",
                        "files": ["string"],
                        "decisions": ["string"],
                        "patterns": ["string"]
                    }
                },
                indexed_fields=[
                    "type",                      # Top-level field
                    "outcome",                   # Top-level field
                    "confidence",                # Top-level field
                    "breadcrumbs.task",          # Nested field
                    "breadcrumbs.decisions[]",   # Array (flattened to string)
                    "breadcrumbs.files[0]"       # First element only
                ]
            )
        
        Benefits:
            - Natural nested queries work efficiently
            - ChromaDB pre-filtering for registered fields
            - Automatic transformation between nested and flat
            - Multiple schemas can coexist
        """
        self.registry.register(schema_name, schema_def, indexed_fields, options)
    
    def get_registered_schemas(self) -> List[str]:
        """Get list of all registered schema names."""
        return self.registry.list_schemas()
    
    # ========== NEW: Generic Metadata Query Capabilities ==========
    
    async def query_by_metadata(self,
                               filters: Dict[str, Any],
                               query: Optional[str] = None,
                               limit: int = 100,
                               order_by: Optional[List[str]] = None,
                               schema_hint: Optional[str] = None) -> List[Message]:
        """
        Query messages by ANY metadata fields using MongoDB-like syntax.
        
        This method is completely agnostic about metadata structure.
        It automatically optimizes queries for registered schemas while
        maintaining full compatibility with unregistered structures.
        
        Args:
            filters: Metadata filters using MongoDB-like operators on ANY fields
            query: Optional semantic search query to combine
            limit: Maximum results
            order_by: List of field paths to sort by
            schema_hint: Optional hint about which schema to use
        
        How it works:
            1. If schema is registered: Automatically translates to ChromaDB fields
            2. If not registered: Falls back to SQL post-filtering
            3. Both paths return identical results (registered is just faster)
        
        Examples:
            # With registered schema - fast ChromaDB pre-filtering
            filters = {
                "breadcrumbs.decisions": {"$contains": "auth"},  # Natural path
                "confidence": {"$gte": 0.8}
            }
            # Automatically uses: reflection__breadcrumbs_decisions in ChromaDB
            
            # Without registered schema - works but uses SQL post-filtering
            filters = {
                "custom.field.path": {"$eq": "value"},
                "another.nested.field": {"$gte": 10}
            }
            # Still works perfectly, just slightly slower
        
        Supported operators:
            $eq, $ne, $gt, $gte, $lt, $lte     - Comparisons
            $in, $nin                           - List membership
            $contains, $not_contains            - String/list contains
            $exists                             - Field existence
            $regex                              - Pattern matching
            $and, $or                           - Logical operators
        """
        # Translate filters based on registered schemas
        chroma_filters, sql_filters = self.translator.translate(
            filters, schema_hint
        )
        
        # Execute optimized query path
        # ... implementation ...
    
    # ========== NEW: Custom Ranking Support ==========
    
    async def search_with_custom_ranker(self,
                                       query: str,
                                       ranker: Callable[[Message, Dict], float],
                                       limit: int = 20,
                                       pre_filter: Optional[Dict] = None,
                                       candidate_multiplier: int = 3) -> List[SearchResult]:
        """
        Search with custom ranking function.
        
        Args:
            query: Semantic search query
            ranker: Function (message, scores) -> float
                   where scores = {
                       "similarity": 0.0-1.0,
                       "confidence": 0.0-1.0, 
                       "age_hours": float,
                       "decay": 0.0-1.0
                   }
            limit: Final result count
            pre_filter: Optional metadata filters
            candidate_multiplier: Fetch N*limit candidates for re-ranking
        
        Example:
            def my_ranker(message: Message, scores: Dict) -> float:
                # Custom logic using message.metadata
                if "security" in message.metadata.get("tags", []):
                    return scores["similarity"] * 1.5
                return scores["similarity"]
            
            results = await api.search_with_custom_ranker(
                query="authentication",
                ranker=my_ranker
            )
        """
        pass
    
    # ========== NEW: Aggregation Capabilities ==========
    
    async def aggregate_by_metadata(self,
                                   group_by: str,
                                   metrics: List[str],
                                   filters: Optional[Dict] = None,
                                   time_window: Optional[str] = None) -> List[AggregationResult]:
        """
        Aggregate messages by ANY metadata field.
        
        The infrastructure doesn't know what these fields represent -
        it just aggregates by the specified JSON path.
        
        Args:
            group_by: ANY metadata field path to group by (supports nested)
            metrics: List of metrics to calculate
                    ["count", "avg_confidence", "max_confidence", 
                     "min_confidence", "unique_senders"]
            filters: Optional metadata filters (ANY fields)
            time_window: Optional time window ("7d", "24h", "1M")
        
        Examples:
            # Group by whatever field the app uses:
            
            # Some app might group by "patterns"
            results = await api.aggregate_by_metadata(
                group_by="metadata.breadcrumbs.patterns",
                metrics=["count", "avg_confidence"]
            )
            
            # Another app might group by completely different fields
            results = await api.aggregate_by_metadata(
                group_by="metadata.department",
                metrics=["count", "unique_senders"]
            )
            
            # Infrastructure doesn't care - just groups by JSON path
        """
        pass
    
    # ========== NEW: Relationship Queries ==========
    
    async def find_related_messages(self,
                                   message_id: int,
                                   relationship_types: List[str],
                                   max_distance: int = 2) -> List[Message]:
        """
        Find messages related to a given message.
        
        Args:
            message_id: Source message
            relationship_types: Types to follow
                ["semantic_similarity", "same_sender", 
                 "same_channel", "temporal_proximity",
                 "shared_metadata"]  # Generic - any shared metadata values
            max_distance: How many hops to follow
        """
        pass
    
    # ========== NEW: Bulk Operations ==========
    
    async def update_metadata(self,
                             message_id: int,
                             metadata_updates: Dict,
                             merge: bool = True) -> None:
        """
        Update metadata for a message with arbitrary JSON.
        
        Args:
            message_id: Message to update
            metadata_updates: Arbitrary JSON to store
            merge: If True, merge with existing; if False, replace
        """
        pass
    
    async def update_metadata_bulk(self,
                                  updates: List[Tuple[int, Dict]]) -> int:
        """
        Update metadata for multiple messages.
        
        Args:
            updates: List of (message_id, metadata_dict) tuples
        
        Returns:
            Number of messages updated
        """
        pass
    
    async def update_confidence_bulk(self,
                                    updates: List[Tuple[int, float]]) -> int:
        """
        Update confidence scores in bulk.
        
        Args:
            updates: List of (message_id, new_confidence)
        
        Returns:
            Number of messages updated
        """
        pass
    
    # ========== Statistics & Monitoring ==========
    
    async def get_statistics(self,
                            time_window: Optional[str] = None) -> Dict:
        """
        Get infrastructure statistics.
        
        Returns:
            {
                "total_messages": int,
                "total_channels": int,
                "avg_confidence": float,
                "messages_with_embeddings": int,
                "storage_size_mb": float,
                "index_size_mb": float,
                "time_window_stats": {...}
            }
        """
        pass
    
    async def get_performance_metrics(self) -> Dict:
        """
        Get performance metrics.
        
        Returns:
            {
                "avg_search_latency_ms": float,
                "avg_store_latency_ms": float,
                "cache_hit_rate": float,
                "embedding_generation_ms": float
            }
        """
        pass
```

### 3. Data Models

```python
# claude_slack/models.py

from dataclasses import dataclass
from typing import Dict, Optional, Any, List
from datetime import datetime

@dataclass
class Message:
    """Message model for API responses."""
    id: int
    channel_id: str
    sender_id: str
    content: str
    timestamp: datetime
    confidence: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None  # Arbitrary JSON - no schema

@dataclass
class SearchResult:
    """Search result with scoring information."""
    message: Message
    scores: Dict[str, float]  # similarity, confidence, decay, final
    
@dataclass  
class AggregationResult:
    """Aggregation result."""
    group_value: Any
    metrics: Dict[str, Any]
```

### 4. Schema Registry Implementation

```python
# claude_slack/registry.py

class SchemaRegistry:
    """
    Manages metadata schemas and their field mappings.
    """
    
    def __init__(self):
        self.schemas = {}
        self.field_mappings = {}     # nested_path -> flat_name
        self.reverse_mappings = {}   # flat_name -> nested_path
    
    def register(self, 
                 schema_name: str,
                 schema_def: Dict,
                 indexed_fields: List[str],
                 options: Optional[Dict] = None):
        """Register a schema with its indexed fields."""
        
        mappings = {}
        for field_path in indexed_fields:
            # Generate deterministic flat names
            flat_name = self._generate_flat_name(schema_name, field_path)
            mappings[field_path] = flat_name
            self.reverse_mappings[flat_name] = (schema_name, field_path)
        
        self.schemas[schema_name] = {
            "definition": schema_def,
            "indexed_fields": indexed_fields,
            "mappings": mappings,
            "options": options or {}
        }
    
    def _generate_flat_name(self, schema: str, path: str) -> str:
        """
        Generate ChromaDB-compatible flat field name.
        Examples:
            ("reflection", "breadcrumbs.task") -> "reflection__breadcrumbs_task"
            ("insight", "impact_score") -> "insight__impact_score"
        """
        clean_path = path.replace(".", "_").replace("[]", "_arr").replace("[", "_").replace("]", "")
        return f"{schema}__{clean_path}"
```

### 5. Schema Transformer Implementation

```python
# claude_slack/transformer.py

class SchemaTransformer:
    """
    Transforms between nested JSON and flat ChromaDB metadata.
    """
    
    def __init__(self, registry: SchemaRegistry):
        self.registry = registry
    
    def to_chromadb(self, 
                    nested_metadata: Dict,
                    schema_name: Optional[str] = None) -> Dict:
        """
        Transform nested metadata to flat ChromaDB format.
        
        Example:
            Input:  {"breadcrumbs": {"task": "auth", "files": ["a.py", "b.py"]}}
            Output: {
                "reflection__breadcrumbs_task": "auth",
                "reflection__breadcrumbs_files_arr": "a.py,b.py",
                "_schema": "reflection"
            }
        """
        if not schema_name:
            schema_name = self._detect_schema(nested_metadata)
        
        if not schema_name or schema_name not in self.registry.schemas:
            # No schema - store minimal metadata
            return {"_raw": json.dumps(nested_metadata)[:500]}
        
        schema = self.registry.schemas[schema_name]
        flat = {"_schema": schema_name}
        
        for field_path, flat_name in schema["mappings"].items():
            value = self._extract_value(nested_metadata, field_path)
            if value is not None:
                # Convert to ChromaDB-compatible format
                if isinstance(value, list):
                    flat[flat_name] = ",".join(str(v) for v in value)
                    flat[f"{flat_name}__len"] = str(len(value))
                else:
                    flat[flat_name] = str(value)
        
        return flat
    
    def from_chromadb(self, flat_metadata: Dict) -> Dict:
        """Reverse transformation for query results."""
        schema_name = flat_metadata.get("_schema")
        if not schema_name:
            return {}
        
        # Reconstruct nested structure
        nested = {}
        for flat_key, value in flat_metadata.items():
            if flat_key.startswith(f"{schema_name}__"):
                schema_name, field_path = self.registry.reverse_mappings.get(flat_key, (None, None))
                if field_path:
                    self._set_value(nested, field_path, value)
        
        return nested
```

### 6. Query Translator Implementation

```python
# claude_slack/translator.py

class QueryTranslator:
    """
    Translates nested queries to ChromaDB and SQL filters.
    """
    
    def __init__(self, registry: SchemaRegistry):
        self.registry = registry
    
    def translate(self,
                 filters: Dict,
                 schema_hint: Optional[str] = None) -> Tuple[Dict, Dict]:
        """
        Translate nested filters to optimized query filters.
        
        Returns:
            (chroma_filters, sql_filters) tuple
        """
        if not schema_hint:
            schema_hint = self._detect_schema_from_filters(filters)
        
        if not schema_hint or schema_hint not in self.registry.schemas:
            # No schema - all filters go to SQL
            return {}, filters
        
        schema = self.registry.schemas[schema_hint]
        chroma_filters = {}
        sql_filters = {}
        
        for field_path, condition in filters.items():
            if field_path in schema["mappings"]:
                # Indexed field - use ChromaDB
                flat_name = schema["mappings"][field_path]
                chroma_filters[flat_name] = self._translate_condition(condition)
            else:
                # Non-indexed - use SQL
                sql_filters[field_path] = condition
        
        return chroma_filters, sql_filters
```

### 7. Metadata Filter Implementation

```python
# claude_slack/filters.py

import json
from typing import Dict, Any, List
from datetime import datetime, timedelta

class MetadataFilter:
    """
    Parse and apply MongoDB-like filters to arbitrary metadata.
    
    This class is completely agnostic about metadata structure.
    It just converts MongoDB-style queries to SQL for ANY JSON fields.
    """
    
    OPERATORS = {
        "$eq", "$ne", "$gt", "$gte", "$lt", "$lte",
        "$in", "$nin", "$contains", "$not_contains",
        "$exists", "$regex", "$and", "$or"
    }
    
    @classmethod
    def parse(cls, filters: Dict) -> str:
        """
        Convert filter dict to SQL WHERE clause.
        Works with ANY metadata structure - no schema assumptions.
        """
        conditions = []
        
        for field, constraint in filters.items():
            if field.startswith("$"):
                # Logical operator
                conditions.append(cls._parse_logical(field, constraint))
            else:
                # Field constraint - could be ANY field path
                conditions.append(cls._parse_field(field, constraint))
        
        return " AND ".join(conditions)
    
    @classmethod
    def _parse_field(cls, field: str, constraint: Any) -> str:
        """
        Parse a single field constraint.
        Field can be ANY path in the metadata JSON.
        """
        # Convert ANY dot notation to JSON path
        json_path = cls._field_to_json_path(field)
        
        if isinstance(constraint, dict):
            # Operator-based constraint
            operator = list(constraint.keys())[0]
            value = constraint[operator]
            
            if operator == "$contains":
                # JSON contains check
                return f"json_extract(metadata, '{json_path}') LIKE '%{value}%'"
            elif operator == "$in":
                # IN clause
                values = ", ".join(f"'{v}'" for v in value)
                return f"json_extract(metadata, '{json_path}') IN ({values})"
            # ... handle other operators
        else:
            # Direct equality
            return f"json_extract(metadata, '{json_path}') = '{constraint}'"
    
    @classmethod
    def _field_to_json_path(cls, field: str) -> str:
        """
        Convert ANY dot notation to JSON path.
        Examples:
            "foo" -> "$.foo"
            "foo.bar" -> "$.foo.bar"
            "deeply.nested.field" -> "$.deeply.nested.field"
        """
        return "$." + field
```

### 5. Integration with Existing HybridStore

```python
# Modify existing hybrid_store.py to support new features

class HybridStore:
    # ... existing code ...
    
    async def query_by_metadata_sql(self, 
                                   sql_where: str,
                                   limit: int = 100) -> List[Dict]:
        """
        Execute metadata query using SQL WHERE clause.
        Used by the API layer for metadata filtering.
        """
        sql = f"""
            SELECT * FROM messages
            WHERE {sql_where}
            ORDER BY timestamp DESC
            LIMIT ?
        """
        cursor = self.conn.execute(sql, (limit,))
        return [dict(row) for row in cursor.fetchall()]
    
    async def aggregate_sql(self,
                           group_field: str,
                           metrics: List[str],
                           where_clause: str = "1=1") -> List[Dict]:
        """
        Execute aggregation query.
        Used by the API layer for pattern analysis.
        """
        # Build SQL aggregation query
        pass
```

### 6. Package Setup

```python
# setup.py

from setuptools import setup, find_packages

setup(
    name="claude-slack",
    version="4.1.0",
    packages=find_packages(),
    install_requires=[
        "aiosqlite>=0.19.0",
        "chromadb>=0.4.22",
        "numpy>=1.24.0",
    ],
    extras_require={
        "mcp": ["mcp>=0.1.0"],
    },
    python_requires=">=3.8",
    author="Your Name",
    description="Infrastructure layer for AI agent knowledge management",
    long_description=open("README.md").read(),
    long_description_content_type="text/markdown",
    url="https://github.com/yourusername/claude-slack",
)
```

## Implementation Priority

### Phase 1: Core API Structure (Week 1)
- [ ] Create `claude_slack` package structure
- [ ] Implement `ClaudeSlackAPI` class shell
- [ ] Define data models
- [ ] Setup package installation

### Phase 2: Metadata Queries (Week 1-2)
- [ ] Implement metadata filter parser
- [ ] Add `query_by_metadata` method
- [ ] Add `find_by_breadcrumbs` convenience method
- [ ] Test with complex queries

### Phase 3: Custom Ranking (Week 2)
- [ ] Implement `search_with_custom_ranker`
- [ ] Allow ranker injection
- [ ] Maintain performance with caching

### Phase 4: Aggregation (Week 2-3)
- [ ] Implement `aggregate_by_metadata`
- [ ] Add time window support
- [ ] Optimize for common patterns

### Phase 5: Testing & Documentation (Week 3)
- [ ] Unit tests for all API methods
- [ ] Integration tests with claude-brain
- [ ] API documentation
- [ ] Usage examples

## Success Criteria

1. **Clean Interface**: External systems only import from `claude_slack` package
2. **No Breaking Changes**: All existing MCP tools continue working
3. **Performance**: Metadata queries < 50ms for 10k messages
4. **Extensibility**: Custom rankers work without modifying claude-slack
5. **Documentation**: Complete API docs with examples

## Migration Path

1. **v4.0 → v4.1**: Add API without changing existing code
2. **MCP Tools**: Continue using existing HybridStore internally
3. **Python API**: New parallel interface for claude-brain
4. **Gradual Adoption**: claude-brain can start using API immediately

## Example Usage from Claude-Brain

```python
from claude_slack import ClaudeSlackAPI

# Initialize API
api = ClaudeSlackAPI(
    db_path="~/.claude/claude-slack/data/claude-slack.db"
)

# STEP 1: Register schemas for optimization (optional but recommended)
api.register_schema(
    "reflection",
    schema_def={
        "type": "string",
        "outcome": "string",
        "confidence": "float",
        "breadcrumbs": {
            "task": "string",
            "files": ["string"],
            "decisions": ["string"],
            "patterns": ["string"]
        }
    },
    indexed_fields=[
        "type",                      # Top-level fields
        "outcome",
        "confidence",
        "breadcrumbs.task",          # Nested field
        "breadcrumbs.decisions[]",   # Array flattened
        "breadcrumbs.patterns[0]"    # First element only
    ]
)

api.register_schema(
    "insight",
    schema_def={
        "type": "string",
        "domain": "string",
        "impact_score": "float",
        "related_insights": ["string"]
    },
    indexed_fields=["type", "domain", "impact_score"]
)

# STEP 2: Store messages with natural nested structure
await api.store_message(
    channel_id="notes:backend-engineer",
    sender_id="backend-engineer",
    content="Successfully implemented JWT auth...",
    metadata={
        "type": "reflection",
        "outcome": "success",
        "confidence": 0.9,
        "breadcrumbs": {
            "task": "implement authentication",
            "files": ["src/auth.py", "tests/test_auth.py"],
            "decisions": ["jwt", "stateless", "RS256"],
            "patterns": ["middleware", "decorator"]
        }
    }
)
# Claude-slack automatically:
# 1. Stores full nested structure in SQLite
# 2. Transforms to flat structure for ChromaDB using registered schema
# 3. Creates searchable embeddings

# STEP 3: Query using natural nested paths
results = await api.query_by_metadata({
    "breadcrumbs.decisions": {"$contains": "jwt"},  # Natural nested path
    "confidence": {"$gte": 0.8},
    "outcome": "success"
})
# Claude-slack automatically:
# 1. Detects this matches "reflection" schema
# 2. Translates to: reflection__breadcrumbs_decisions for ChromaDB
# 3. Pre-filters in ChromaDB (fast!)
# 4. Returns results with natural nested structure

# Works even WITHOUT schema registration (just slower)
custom_results = await api.query_by_metadata({
    "custom.unregistered.field": {"$eq": "value"},
    "another.new.path": {"$gte": 10}
})
# Falls back to SQL post-filtering - still works perfectly!

# Custom ranking with metadata access
def expertise_ranker(message, scores):
    """Custom ranker that understands our metadata conventions."""
    # We know our schema, infrastructure doesn't
    if message.metadata.get("breadcrumbs", {}).get("patterns"):
        if "middleware" in message.metadata["breadcrumbs"]["patterns"]:
            return scores["similarity"] * 1.3
    return scores["similarity"]

expert_solutions = await api.search_with_custom_ranker(
    query="authentication implementation",
    ranker=expertise_ranker
)

# Aggregate by any metadata field
patterns = await api.aggregate_by_metadata(
    group_by="breadcrumbs.patterns",  # Natural path
    metrics=["count", "avg_confidence"],
    time_window="7d"
)
# Automatically uses optimized path if schema registered
```

## Key Innovation: Schema Registry with Transparent Transformation

The schema registry approach solves the fundamental tension between:
- **Schema-agnostic infrastructure** (flexibility)
- **Optimal query performance** (efficiency)

### How It Works

1. **Apps register their schemas** with indexed fields
2. **Infrastructure creates mappings** between nested and flat structures
3. **Queries automatically translate** to optimal storage format
4. **Results transform back** to natural nested structure
5. **Unregistered schemas still work** (fallback to SQL)

### Benefits

1. **Clean Separation**: Claude-slack remains unopinionated about schemas
2. **Natural Interface**: Apps use their preferred nested structures  
3. **Optimal Performance**: Registered fields get ChromaDB pre-filtering
4. **Multiple Schemas**: Different apps can register different schemas
5. **Gradual Adoption**: Start without schemas, add them for performance
6. **Future Proof**: New schemas don't require infrastructure changes

### Technical Advantages

- **No impedance mismatch**: Apps never see flat ChromaDB fields
- **Deterministic naming**: Flat field names are predictable and stable
- **Bidirectional mapping**: Query and results both use natural paths
- **Schema detection**: Can auto-detect which schema applies
- **Partial indexing**: Only index the fields that matter for search

## Conclusion

This implementation plan provides claude-slack with a sophisticated yet unopinionated infrastructure API. The schema registry innovation allows apps to work with their natural data structures while the infrastructure transparently optimizes storage and retrieval. This creates the perfect foundation for intelligence layers like claude-brain to build upon, maintaining clean separation of concerns while achieving optimal performance.